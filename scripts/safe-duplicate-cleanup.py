#!/usr/bin/env python3
"""
O3æ¨å¥¨ã®å®‰å…¨æªç½®ã‚’å®Ÿè£…ã—ãŸé‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
SHA-256æ¤œè¨¼ + å‚ç…§ã‚¹ã‚­ãƒ£ãƒ³ + æ®µéšçš„éš”é›¢
"""

import os
import shutil
import hashlib
import subprocess
from pathlib import Path
from typing import Dict, List, Set, Tuple
import json
from datetime import datetime
import re

class SafeDuplicateCleanup:
    """O3æ¨å¥¨ã®å®‰å…¨æªç½®ã‚’å®Ÿè£…ã—ãŸé‡è¤‡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.quarantine_dir = self.project_root / ".archive" / "duplicates" / f"{datetime.now().strftime('%Y%m%d')}"
        
        # ä¿è­·å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.protected_dirs = {
            "src", "docs", ".git", "runtime", "data", "models", 
            "api", "compliance", ".dev", "tests", "scripts"
        }
        
        # æ¤œç´¢å¯¾è±¡æ‹¡å¼µå­ï¼ˆå‚ç…§ã‚¹ã‚­ãƒ£ãƒ³ç”¨ï¼‰
        self.code_extensions = {
            "*.py", "*.sh", "*.js", "*.ts", "*.json", "*.yaml", 
            "*.yml", "*.md", "*.toml", "*.cfg", "*.ini"
        }
        
    def calculate_file_hash(self, file_path: Path) -> str:
        """SHA-256ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        sha256_hash = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except Exception as e:
            print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""
    
    def find_true_duplicates(self) -> Dict[str, List[str]]:
        """SHA-256ãƒ™ãƒ¼ã‚¹ã®çœŸã®é‡è¤‡æ¤œå‡º"""
        print("ğŸ” SHA-256ãƒ™ãƒ¼ã‚¹é‡è¤‡ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹...")
        
        hash_groups = {}
        total_files = 0
        
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        for file_path in self.project_root.rglob("*"):
            if not file_path.is_file():
                continue
                
            # ä¿è­·å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ã‚¹ã‚­ãƒƒãƒ—
            if any(protected in file_path.parts for protected in self.protected_dirs):
                continue
                
            # éš ã—ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
            if file_path.name.startswith('.') and file_path.name not in ['.gitignore', '.editorconfig']:
                continue
                
            file_hash = self.calculate_file_hash(file_path)
            if file_hash:
                if file_hash not in hash_groups:
                    hash_groups[file_hash] = []
                hash_groups[file_hash].append(str(file_path))
                total_files += 1
        
        # é‡è¤‡ã®ã¿æŠ½å‡º
        true_duplicates = {
            hash_val: paths for hash_val, paths in hash_groups.items() 
            if len(paths) > 1
        }
        
        print(f"ğŸ“Š ã‚¹ã‚­ãƒ£ãƒ³çµæœ: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«, {len(true_duplicates)}å€‹ã®ãƒãƒƒã‚·ãƒ¥ã‚°ãƒ«ãƒ¼ãƒ—ã§é‡è¤‡")
        return true_duplicates
    
    def scan_code_references(self, file_paths: List[str]) -> Dict[str, List[Dict]]:
        """ã‚³ãƒ¼ãƒ‰å†…ã®å‚ç…§ã‚¹ã‚­ãƒ£ãƒ³"""
        print("ğŸ” ã‚³ãƒ¼ãƒ‰å‚ç…§ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹...")
        
        references = {}
        
        for file_path in file_paths:
            file_name = Path(file_path).name
            relative_path = str(Path(file_path).relative_to(self.project_root))
            
            # ripgrepã§å‚ç…§ã‚’æ¤œç´¢
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã§ã®æ¤œç´¢
                result = subprocess.run([
                    "rg", "--json", "--type-add", "code:*.{py,sh,js,ts,json,yaml,yml,md,toml,cfg,ini}",
                    "--type", "code", file_name
                ], capture_output=True, text=True, cwd=self.project_root)
                
                if result.returncode == 0:
                    refs = []
                    for line in result.stdout.strip().split('\n'):
                        if line:
                            try:
                                data = json.loads(line)
                                if data.get('type') == 'match':
                                    refs.append({
                                        'file': data['data']['path']['text'],
                                        'line': data['data']['line_number'],
                                        'content': data['data']['lines']['text'].strip()
                                    })
                            except:
                                continue
                    
                    if refs:
                        references[file_path] = refs
                        
            except Exception as e:
                print(f"âš ï¸ å‚ç…§ã‚¹ã‚­ãƒ£ãƒ³ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        return references
    
    def select_canonical_files(self, duplicates: Dict[str, List[str]]) -> Dict[str, Dict]:
        """é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰ä¿æŒã™ã¹ããƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ"""
        cleanup_plan = {}
        
        for hash_val, file_paths in duplicates.items():
            if len(file_paths) < 2:
                continue
                
            # å„ªå…ˆé †ä½ãƒ«ãƒ¼ãƒ«
            def priority_score(path: str) -> int:
                path_obj = Path(path)
                score = 0
                
                # 1. ã‚ˆã‚Šæµ…ã„éšå±¤ã‚’å„ªå…ˆ
                score += 100 - len(path_obj.parts)
                
                # 2. ç‰¹å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å„ªå…ˆ
                if 'docs/' in path:
                    score += 50
                elif 'src/' in path:
                    score += 40
                elif 'scripts/' in path:
                    score += 30
                
                # 3. READMEã¯è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å„ªå…ˆ
                if path_obj.name == 'README.md':
                    score += 20
                
                # 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–
                if any(keyword in path.lower() for keyword in ['backup', 'old', 'copy', 'temp']):
                    score -= 100
                
                return score
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ­£è¦ç‰ˆã¨ã—ã¦é¸æŠ
            sorted_paths = sorted(file_paths, key=priority_score, reverse=True)
            canonical = sorted_paths[0]
            duplicates_to_remove = sorted_paths[1:]
            
            cleanup_plan[hash_val] = {
                'canonical': canonical,
                'duplicates': duplicates_to_remove,
                'file_count': len(file_paths),
                'size_bytes': Path(canonical).stat().st_size if Path(canonical).exists() else 0
            }
        
        return cleanup_plan
    
    def create_quarantine_plan(self) -> Dict:
        """éš”é›¢è¨ˆç”»ä½œæˆ"""
        print("ğŸ“‹ å®‰å…¨ãªéš”é›¢è¨ˆç”»ä½œæˆä¸­...")
        
        # 1. çœŸã®é‡è¤‡æ¤œå‡º
        true_duplicates = self.find_true_duplicates()
        
        # 2. æ­£è¦ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
        cleanup_plan = self.select_canonical_files(true_duplicates)
        
        # 3. å‰Šé™¤äºˆå®šãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆä½œæˆ
        files_to_quarantine = []
        for group in cleanup_plan.values():
            files_to_quarantine.extend(group['duplicates'])
        
        # 4. å‚ç…§ã‚¹ã‚­ãƒ£ãƒ³
        references = self.scan_code_references(files_to_quarantine)
        
        # 5. æœ€çµ‚è¨ˆç”»
        plan = {
            "created": datetime.now().isoformat(),
            "total_duplicate_groups": len(cleanup_plan),
            "total_files_to_quarantine": len(files_to_quarantine),
            "total_size_mb": sum(group['size_bytes'] for group in cleanup_plan.values()) / 1024 / 1024,
            "cleanup_plan": cleanup_plan,
            "code_references": references,
            "quarantine_directory": str(self.quarantine_dir),
            "safety_status": self._assess_safety(references)
        }
        
        return plan
    
    def _assess_safety(self, references: Dict) -> Dict:
        """å®‰å…¨æ€§è©•ä¾¡"""
        total_refs = sum(len(refs) for refs in references.values())
        
        if total_refs == 0:
            status = "SAFE"
            risk_level = "LOW"
        elif total_refs < 5:
            status = "CAUTION" 
            risk_level = "MEDIUM"
        else:
            status = "REVIEW_REQUIRED"
            risk_level = "HIGH"
        
        return {
            "status": status,
            "risk_level": risk_level,
            "total_references": total_refs,
            "referenced_files": len(references),
            "recommendation": self._get_safety_recommendation(status)
        }
    
    def _get_safety_recommendation(self, status: str) -> str:
        """å®‰å…¨æ€§æ¨å¥¨"""
        recommendations = {
            "SAFE": "éš”é›¢å®Ÿè¡Œå¯èƒ½ã€‚å‚ç…§ãªã—ã§å®‰å…¨ã§ã™ã€‚",
            "CAUTION": "å°‘æ•°ã®å‚ç…§ã‚ã‚Šã€‚æ‰‹å‹•ç¢ºèªå¾Œã«éš”é›¢å®Ÿè¡Œã€‚",
            "REVIEW_REQUIRED": "å¤šæ•°ã®å‚ç…§ã‚ã‚Šã€‚å„å‚ç…§ã‚’è©³ç´°ç¢ºèªå¾Œã«æ®µéšçš„å®Ÿè¡Œã€‚"
        }
        return recommendations.get(status, "è¦è©³ç´°åˆ†æ")
    
    def execute_quarantine(self, plan: Dict, dry_run: bool = True) -> Dict:
        """éš”é›¢å®Ÿè¡Œ"""
        if dry_run:
            print("ğŸ” [DRY RUN] éš”é›¢ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        else:
            print("ğŸ“¦ éš”é›¢å®Ÿè¡Œé–‹å§‹")
            self.quarantine_dir.mkdir(parents=True, exist_ok=True)
        
        results = {
            "quarantined_files": [],
            "errors": [],
            "total_freed_mb": 0,
            "dry_run": dry_run
        }
        
        for hash_val, group in plan["cleanup_plan"].items():
            for duplicate_path in group["duplicates"]:
                try:
                    source = Path(duplicate_path)
                    if not source.exists():
                        continue
                    
                    if dry_run:
                        print(f"ğŸ” [DRY RUN] éš”é›¢äºˆå®š: {duplicate_path}")
                        results["quarantined_files"].append(duplicate_path)
                        results["total_freed_mb"] += group["size_bytes"] / 1024 / 1024
                    else:
                        # éš”é›¢å…ˆãƒ‘ã‚¹è¨ˆç®—
                        relative_path = source.relative_to(self.project_root)
                        quarantine_target = self.quarantine_dir / relative_path
                        
                        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                        quarantine_target.parent.mkdir(parents=True, exist_ok=True)
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•
                        shutil.move(str(source), str(quarantine_target))
                        print(f"ğŸ“¦ éš”é›¢å®Œäº†: {duplicate_path}")
                        
                        results["quarantined_files"].append(duplicate_path)
                        results["total_freed_mb"] += group["size_bytes"] / 1024 / 1024
                
                except Exception as e:
                    error_msg = f"âŒ éš”é›¢ã‚¨ãƒ©ãƒ¼ {duplicate_path}: {e}"
                    print(error_msg)
                    results["errors"].append(error_msg)
        
        return results
    
    def save_plan(self, plan: Dict, results: Dict = None):
        """è¨ˆç”»ä¿å­˜"""
        runtime_dir = self.project_root / "runtime"
        runtime_dir.mkdir(exist_ok=True)
        
        plan_file = runtime_dir / f"duplicate-cleanup-plan-{datetime.now().strftime('%Y%m%d-%H%M%S')}.json"
        
        full_report = {
            "plan": plan,
            "execution_results": results,
            "o3_recommendations_implemented": [
                "SHA-256 hash verification",
                "Code reference scanning",
                "Quarantine phase implementation",
                "Protected directory exclusion",
                "Safety assessment"
            ]
        }
        
        with open(plan_file, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ è¨ˆç”»ä¿å­˜: {plan_file}")
        return plan_file

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    cleanup = SafeDuplicateCleanup()
    
    print("ğŸ›¡ï¸ O3æ¨å¥¨å®‰å…¨æªç½®ä»˜ãé‡è¤‡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")
    print("=" * 60)
    
    # éš”é›¢è¨ˆç”»ä½œæˆ
    plan = cleanup.create_quarantine_plan()
    
    # çµæœè¡¨ç¤º
    print(f"\nğŸ“Š éš”é›¢è¨ˆç”»:")
    print(f"  é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—: {plan['total_duplicate_groups']}")
    print(f"  éš”é›¢å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {plan['total_files_to_quarantine']}")
    print(f"  è§£æ”¾äºˆå®šå®¹é‡: {plan['total_size_mb']:.2f}MB")
    print(f"  ã‚³ãƒ¼ãƒ‰å‚ç…§: {plan['safety_status']['total_references']}ä»¶")
    print(f"  å®‰å…¨æ€§: {plan['safety_status']['status']} ({plan['safety_status']['risk_level']})")
    print(f"  æ¨å¥¨: {plan['safety_status']['recommendation']}")
    
    # ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³
    print(f"\nğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³å®Ÿè¡Œ...")
    dry_results = cleanup.execute_quarantine(plan, dry_run=True)
    
    # è¨ˆç”»ä¿å­˜
    plan_file = cleanup.save_plan(plan, dry_results)
    
    # å®Ÿè¡Œåˆ¤å®š
    if plan['safety_status']['status'] == 'SAFE':
        print(f"\nâœ… å®‰å…¨æ€§ç¢ºèª: éš”é›¢å®Ÿè¡Œå¯èƒ½")
        print(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: python3 {__file__} --execute")
    else:
        print(f"\nâš ï¸ è¦ç¢ºèª: {plan['safety_status']['recommendation']}")
        print(f"è©³ç´°ç¢ºèª: {plan_file}")
    
    print(f"\nğŸ¯ O3æ¨å¥¨æªç½®å®Œäº†:")
    print("  âœ… SHA-256ãƒãƒƒã‚·ãƒ¥æ¤œè¨¼")
    print("  âœ… ã‚³ãƒ¼ãƒ‰å‚ç…§ã‚¹ã‚­ãƒ£ãƒ³")  
    print("  âœ… æ®µéšçš„éš”é›¢è¨ˆç”»")
    print("  âœ… å®‰å…¨æ€§è©•ä¾¡")

if __name__ == "__main__":
    import sys
    
    if "--execute" in sys.argv:
        cleanup = SafeDuplicateCleanup()
        plan = cleanup.create_quarantine_plan()
        
        if plan['safety_status']['status'] == 'SAFE':
            print("ğŸš€ éš”é›¢å®Ÿè¡Œé–‹å§‹...")
            results = cleanup.execute_quarantine(plan, dry_run=False)
            cleanup.save_plan(plan, results)
            print("âœ… éš”é›¢å®Œäº†")
        else:
            print("âŒ å®‰å…¨æ€§ç¢ºèªãŒå¿…è¦ã§ã™ã€‚æ‰‹å‹•ç¢ºèªå¾Œã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        main()