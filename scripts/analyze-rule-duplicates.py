#!/usr/bin/env python3
"""
ãƒ«ãƒ¼ãƒ«é‡è¤‡åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
docs/ã¨codeå†…ã®ãƒ«ãƒ¼ãƒ«ãƒ»ãƒŠãƒ¬ãƒƒã‚¸ã®é‡è¤‡ã‚’æ¤œå‡ºã—çµ±åˆææ¡ˆã‚’ç”Ÿæˆ
"""

import os
import re
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple
import json

class RuleDuplicateAnalyzer:
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.rule_files = []
        self.content_hashes = {}
        self.duplicates = []
        
    def find_rule_files(self) -> List[Path]:
        """ãƒ«ãƒ¼ãƒ«é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        patterns = [
            "**/*.mdc",
            "**/CLAUDE.md", 
            "**/README.md",
            "**/*rules*",
            "**/*guideline*",
            "**/*standard*"
        ]
        
        rule_files = []
        for pattern in patterns:
            rule_files.extend(self.project_root.glob(pattern))
            
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        excludes = {".git", "node_modules", "__pycache__", ".pytest_cache"}
        
        return [f for f in rule_files 
                if not any(exc in str(f) for exc in excludes)]
    
    def extract_rules(self, file_path: Path) -> List[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ«ãƒ¼ãƒ«è¨˜è¿°ã‚’æŠ½å‡º"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"âŒ Error reading {file_path}: {e}")
            return []
        
        # ãƒ«ãƒ¼ãƒ«è¨˜è¿°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        rule_patterns = [
            r'[#*-] .{10,}',  # ãƒªã‚¹ãƒˆé …ç›®
            r'```[^`]+```',   # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯
            r'> .{10,}',      # å¼•ç”¨
            r'## .{5,}',      # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼
        ]
        
        rules = []
        for pattern in rule_patterns:
            matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
            rules.extend(matches)
            
        return [self.normalize_rule(rule) for rule in rules if len(rule.strip()) > 10]
    
    def normalize_rule(self, rule: str) -> str:
        """ãƒ«ãƒ¼ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’æ­£è¦åŒ–"""
        # è¨˜å·ãƒ»ç©ºç™½ã®æ­£è¦åŒ–
        normalized = re.sub(r'[#*-] ', '', rule)
        normalized = re.sub(r'\s+', ' ', normalized.strip())
        normalized = normalized.lower()
        
        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®å¤‰æ•°ã‚’çµ±ä¸€
        normalized = re.sub(r'\$\{[^}]+\}', '${VAR}', normalized)
        normalized = re.sub(r'\w+\.\w+', 'file.ext', normalized)
        
        return normalized
    
    def calculate_similarity(self, rule1: str, rule2: str) -> float:
        """2ã¤ã®ãƒ«ãƒ¼ãƒ«é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        from difflib import SequenceMatcher
        return SequenceMatcher(None, rule1, rule2).ratio()
    
    def find_duplicates(self, threshold: float = 0.8) -> List[Tuple]:
        """é‡è¤‡ãƒ«ãƒ¼ãƒ«ã‚’æ¤œå‡ºï¼ˆåŠ¹ç‡åŒ–ç‰ˆï¼‰"""
        all_rules = {}
        
        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ«ãƒ¼ãƒ«ã‚’åé›†ï¼ˆã‚µã‚¤ã‚ºåˆ¶é™ï¼‰
        total_rules = 0
        for file_path in self.rule_files:
            if total_rules > 1000:  # å‡¦ç†åˆ¶é™
                print(f"âš ï¸ ãƒ«ãƒ¼ãƒ«æ•°åˆ¶é™åˆ°é”ã€å‡¦ç†ã‚’åˆ¶é™ã—ã¾ã™")
                break
                
            rules = self.extract_rules(file_path)
            # é•·ã™ãã‚‹ãƒ«ãƒ¼ãƒ«ã¯é™¤å¤–
            rules = [r for r in rules if len(r) < 500]
            all_rules[file_path] = rules[:20]  # ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šæœ€å¤§20ãƒ«ãƒ¼ãƒ«
            total_rules += len(rules)
            
            print(f"ğŸ“„ {file_path.name}: {len(rules)}ãƒ«ãƒ¼ãƒ«")
        
        duplicates = []
        
        # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        rule_hashes = {}
        for file_path, rules in all_rules.items():
            for rule in rules:
                rule_hash = hashlib.md5(rule.encode()).hexdigest()[:8]
                if rule_hash not in rule_hashes:
                    rule_hashes[rule_hash] = []
                rule_hashes[rule_hash].append((file_path, rule))
        
        # åŒä¸€ãƒãƒƒã‚·ãƒ¥ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿è©³ç´°æ¯”è¼ƒ
        processed_pairs = set()
        
        for hash_val, rule_list in rule_hashes.items():
            if len(rule_list) < 2:
                continue
                
            # ã‚°ãƒ«ãƒ¼ãƒ—å†…æ¯”è¼ƒ
            for i in range(len(rule_list)):
                for j in range(i + 1, len(rule_list)):
                    file1, rule1 = rule_list[i]
                    file2, rule2 = rule_list[j]
                    
                    # åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–
                    if file1 == file2:
                        continue
                        
                    pair_key = tuple(sorted([str(file1), str(file2), rule1[:50], rule2[:50]]))
                    if pair_key in processed_pairs:
                        continue
                    processed_pairs.add(pair_key)
                    
                    similarity = self.calculate_similarity(rule1, rule2)
                    
                    if similarity >= threshold:
                        duplicates.append({
                            'similarity': similarity,
                            'file1': str(file1),
                            'file2': str(file2),
                            'rule1': rule1[:100] + "..." if len(rule1) > 100 else rule1,
                            'rule2': rule2[:100] + "..." if len(rule2) > 100 else rule2,
                        })
        
        return duplicates
    
    def generate_consolidation_plan(self) -> Dict:
        """çµ±åˆè¨ˆç”»ã‚’ç”Ÿæˆ"""
        plan = {
            'current_files': len(self.rule_files),
            'duplicates_found': len(self.duplicates),
            'recommendations': []
        }
        
        # é‡è¤‡åº¦ã®é«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ãƒšã‚¢ã‚’ç‰¹å®š
        file_pairs = {}
        for dup in self.duplicates:
            pair = tuple(sorted([dup['file1'], dup['file2']]))
            if pair not in file_pairs:
                file_pairs[pair] = []
            file_pairs[pair].append(dup['similarity'])
        
        # çµ±åˆæ¨å¥¨
        for (file1, file2), similarities in file_pairs.items():
            avg_similarity = sum(similarities) / len(similarities)
            if avg_similarity > 0.7:
                plan['recommendations'].append({
                    'action': 'merge',
                    'files': [file1, file2],
                    'avg_similarity': avg_similarity,
                    'duplicate_count': len(similarities)
                })
        
        return plan
    
    def run_analysis(self) -> Dict:
        """å…¨ä½“åˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸ” ãƒ«ãƒ¼ãƒ«é‡è¤‡åˆ†æé–‹å§‹...")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        self.rule_files = self.find_rule_files()
        print(f"ğŸ“ ç™ºè¦‹ãƒ•ã‚¡ã‚¤ãƒ«: {len(self.rule_files)}")
        
        # é‡è¤‡æ¤œå‡º
        self.duplicates = self.find_duplicates()
        print(f"ğŸ” é‡è¤‡æ¤œå‡º: {len(self.duplicates)}ä»¶")
        
        # çµ±åˆè¨ˆç”»ç”Ÿæˆ
        plan = self.generate_consolidation_plan()
        
        return {
            'files_analyzed': [str(f) for f in self.rule_files],
            'duplicates': self.duplicates,
            'consolidation_plan': plan
        }

def main():
    analyzer = RuleDuplicateAnalyzer()
    results = analyzer.run_analysis()
    
    # çµæœã‚’JSONã§ä¿å­˜
    output_file = "runtime/rule-analysis-results.json"
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“Š åˆ†æçµæœã‚’ä¿å­˜: {output_file}")
    
    # é‡è¦ãªç™ºè¦‹ã‚’ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    duplicates = results['duplicates']
    plan = results['consolidation_plan']
    
    print("\nğŸ“‹ åˆ†æã‚µãƒãƒªãƒ¼:")
    print(f"  å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(results['files_analyzed'])}")
    print(f"  é‡è¤‡æ¤œå‡º: {len(duplicates)}ä»¶")
    print(f"  çµ±åˆæ¨å¥¨: {len(plan['recommendations'])}ãƒšã‚¢")
    
    if duplicates:
        print("\nğŸ” ä¸»è¦ãªé‡è¤‡:")
        for dup in sorted(duplicates, key=lambda x: x['similarity'], reverse=True)[:5]:
            print(f"  {dup['similarity']:.2f}: {Path(dup['file1']).name} â†” {Path(dup['file2']).name}")
    
    if plan['recommendations']:
        print("\nğŸ’¡ çµ±åˆæ¨å¥¨:")
        for rec in plan['recommendations']:
            files = [Path(f).name for f in rec['files']]
            print(f"  {files[0]} + {files[1]} (é¡ä¼¼åº¦: {rec['avg_similarity']:.2f})")

if __name__ == "__main__":
    main()