#!/usr/bin/env python3
"""
AIæ°¸ç¶šè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ  - å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰åŒæœŸ
ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“è¨˜æ†¶ç¶™ç¶šã®æ ¹æœ¬è§£æ±º
"""

import json
import os
import datetime
from pathlib import Path
import hashlib
import sqlite3
from typing import Dict, List, Any

class AIMemorySystem:
    def __init__(self, base_path: str = "claude-memory"):
        self.base_path = Path(base_path)
        self.setup_directories()
        self.init_database()
        
    def setup_directories(self):
        """å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ"""
        dirs = [
            "session-records",
            "conversation-history", 
            "indexed-knowledge",
            "cloud-sync",
            "ai-collaboration",
            "mistake-prevention"
        ]
        for dir_name in dirs:
            (self.base_path / dir_name).mkdir(parents=True, exist_ok=True)
    
    def init_database(self):
        """è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        db_path = self.base_path / "memory.db"
        self.conn = sqlite3.connect(str(db_path))
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜éŒ²ãƒ†ãƒ¼ãƒ–ãƒ«
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS sessions (
                id TEXT PRIMARY KEY,
                start_time TEXT,
                end_time TEXT,
                mistake_count INTEGER,
                tasks_completed TEXT,
                important_learnings TEXT,
                user_context TEXT
            )
        """)
        
        # ä¼šè©±ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS conversations (
                id TEXT PRIMARY KEY,
                session_id TEXT,
                timestamp TEXT,
                user_message TEXT,
                ai_response TEXT,
                keywords TEXT,
                importance_score INTEGER
            )
        """)
        
        # ãƒŸã‚¹é˜²æ­¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS mistake_patterns (
                id INTEGER PRIMARY KEY,
                mistake_type TEXT,
                description TEXT,
                prevention_rule TEXT,
                occurrence_count INTEGER,
                last_occurred TEXT
            )
        """)
        
        self.conn.commit()
    
    def save_session_memory(self, session_data: Dict[str, Any]):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ã‚’æ°¸ç¶šåŒ–"""
        session_id = session_data.get('session_id', self.generate_session_id())
        
        self.conn.execute("""
            INSERT OR REPLACE INTO sessions 
            (id, start_time, mistake_count, tasks_completed, important_learnings, user_context)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            session_id,
            session_data.get('start_time', datetime.datetime.now().isoformat()),
            session_data.get('mistake_count', 78),
            json.dumps(session_data.get('tasks_completed', [])),
            json.dumps(session_data.get('important_learnings', [])),
            json.dumps(session_data.get('user_context', {}))
        ))
        
        self.conn.commit()
        return session_id
    
    def index_conversation(self, user_msg: str, ai_response: str, session_id: str = None):
        """ä¼šè©±ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã¦æ¤œç´¢å¯èƒ½ã«ã™ã‚‹"""
        conv_id = hashlib.md5(f"{user_msg}{ai_response}".encode()).hexdigest()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        keywords = self.extract_keywords(user_msg + " " + ai_response)
        
        # é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        importance = self.calculate_importance(user_msg, ai_response)
        
        self.conn.execute("""
            INSERT OR REPLACE INTO conversations
            (id, session_id, timestamp, user_message, ai_response, keywords, importance_score)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            conv_id,
            session_id or "current",
            datetime.datetime.now().isoformat(),
            user_msg,
            ai_response,
            json.dumps(keywords),
            importance
        ))
        
        self.conn.commit()
    
    def extract_keywords(self, text: str) -> List[str]:
        """é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        important_terms = [
            "è¨˜æ†¶", "ç¶™ç¶š", "ã‚»ãƒƒã‚·ãƒ§ãƒ³", "AI", "ãƒŸã‚¹", "é˜²æ­¢",
            "hooks", "gemini", "o3", "claude", "å®Ÿè£…", "ã‚·ã‚¹ãƒ†ãƒ ",
            "å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸", "ã‚¯ãƒ©ã‚¦ãƒ‰", "åŒæœŸ", "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹",
            "ãƒ—ãƒ¬ã‚¸ãƒ‡ãƒ³ãƒˆ", "è·å‹™", "å®£è¨€", "ãƒã‚§ãƒƒã‚¯"
        ]
        
        text_lower = text.lower()
        found_keywords = [term for term in important_terms if term in text_lower]
        return found_keywords
    
    def calculate_importance(self, user_msg: str, ai_response: str) -> int:
        """é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ1-10ï¼‰"""
        high_importance_indicators = [
            "é‡è¦", "å¿…é ˆ", "çµ¶å¯¾", "ç¦æ­¢", "ã‚¨ãƒ©ãƒ¼", "å•é¡Œ",
            "å®£è¨€", "å¿˜ã‚Œã‚‹", "è¨˜æ†¶", "ç¶™ç¶š", "ã‚·ã‚¹ãƒ†ãƒ "
        ]
        
        text = (user_msg + " " + ai_response).lower()
        score = 5  # åŸºæœ¬ã‚¹ã‚³ã‚¢
        
        for indicator in high_importance_indicators:
            if indicator in text:
                score += 1
        
        return min(score, 10)
    
    def retrieve_context(self, keywords: List[str] = None, limit: int = 10) -> List[Dict]:
        """éå»ã®æ–‡è„ˆã‚’æ¤œç´¢ãƒ»å–å¾—"""
        if keywords:
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹æ¤œç´¢
            keyword_filter = " OR ".join([f"keywords LIKE '%{kw}%'" for kw in keywords])
            query = f"""
                SELECT * FROM conversations 
                WHERE {keyword_filter}
                ORDER BY importance_score DESC, timestamp DESC 
                LIMIT ?
            """
            cursor = self.conn.execute(query, (limit,))
        else:
            # æœ€æ–°ã®é‡è¦ãªä¼šè©±ã‚’å–å¾—
            cursor = self.conn.execute("""
                SELECT * FROM conversations 
                ORDER BY importance_score DESC, timestamp DESC 
                LIMIT ?
            """, (limit,))
        
        return [dict(row) for row in cursor.fetchall()]
    
    def record_mistake_prevention(self, mistake_type: str, description: str, prevention_rule: str):
        """ãƒŸã‚¹é˜²æ­¢ãƒ«ãƒ¼ãƒ«ã‚’è¨˜éŒ²"""
        self.conn.execute("""
            INSERT OR REPLACE INTO mistake_patterns
            (mistake_type, description, prevention_rule, occurrence_count, last_occurred)
            VALUES (?, ?, ?, 
                COALESCE((SELECT occurrence_count FROM mistake_patterns WHERE mistake_type = ?) + 1, 1),
                ?)
        """, (mistake_type, description, prevention_rule, mistake_type, datetime.datetime.now().isoformat()))
        
        self.conn.commit()
    
    def get_startup_context(self) -> Dict[str, Any]:
        """èµ·å‹•æ™‚ã®æ–‡è„ˆæƒ…å ±ã‚’å–å¾—"""
        # æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±
        latest_session = self.conn.execute("""
            SELECT * FROM sessions ORDER BY start_time DESC LIMIT 1
        """).fetchone()
        
        # é‡è¦ãªä¼šè©±å±¥æ­´
        important_conversations = self.retrieve_context(limit=5)
        
        # ãƒŸã‚¹é˜²æ­¢ãƒ«ãƒ¼ãƒ«
        prevention_rules = self.conn.execute("""
            SELECT * FROM mistake_patterns ORDER BY occurrence_count DESC LIMIT 10
        """).fetchall()
        
        return {
            "latest_session": dict(latest_session) if latest_session else None,
            "important_conversations": important_conversations,
            "prevention_rules": [dict(rule) for rule in prevention_rules],
            "current_task": "AIæ°¸ç¶šè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ç¶™ç¶š"
        }
    
    def generate_session_id(self) -> str:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³IDç”Ÿæˆ"""
        return f"session-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}"

# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    memory = AIMemorySystem()
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ä¿å­˜
    session_data = {
        "session_id": memory.generate_session_id(),
        "mistake_count": 78,
        "tasks_completed": ["æ°¸ç¶šè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ", "å¤–éƒ¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å®Ÿè£…é–‹å§‹"],
        "important_learnings": [
            "AIã‚»ãƒƒã‚·ãƒ§ãƒ³é–“è¨˜æ†¶ç¶™ç¶šã¯é‡å¤§ãªç¤¾ä¼šèª²é¡Œ",
            "å®£è¨€å¿˜ã‚Œã¯78å›ãƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸€ã¤",
            "ä½œæ¥­ä¸­æ–­ã›ãšä¸¦è¡Œå‡¦ç†ãŒé‡è¦"
        ],
        "user_context": {
            "project": "AI Compliance Engine",
            "role": "PRESIDENT", 
            "urgent_task": "è¨˜æ†¶ç¶™ç¶šã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…"
        }
    }
    
    session_id = memory.save_session_memory(session_data)
    print(f"âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ä¿å­˜å®Œäº†: {session_id}")
    
    # ä¼šè©±ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
    memory.index_conversation(
        "AIã‚»ãƒƒã‚·ãƒ§ãƒ³é–“è¨˜æ†¶ç¶™ç¶šå•é¡Œã®è§£æ±ºè¦æ±‚",
        "æ°¸ç¶šè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…ã§æ ¹æœ¬è§£æ±ºã—ã¾ã™",
        session_id
    )
    
    # ãƒŸã‚¹é˜²æ­¢è¨˜éŒ²
    memory.record_mistake_prevention(
        "å®£è¨€å¿˜ã‚Œ",
        "å¿…é ˆå®£è¨€ã‚’å¿˜ã‚Œã¦ä½œæ¥­é–‹å§‹",
        "ä½œæ¥­é–‹å§‹å‰ã«å¿…ãšCLAUDE.mdç¢ºèªã¨å®£è¨€å®Ÿè¡Œ"
    )
    
    # èµ·å‹•æ™‚æ–‡è„ˆå–å¾—
    context = memory.get_startup_context()
    print("ğŸ§  èµ·å‹•æ™‚æ–‡è„ˆæƒ…å ±å–å¾—å®Œäº†")
    
    print("âœ… AIæ°¸ç¶šè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ åŸºæœ¬æ©Ÿèƒ½å®Ÿè£…å®Œäº†")