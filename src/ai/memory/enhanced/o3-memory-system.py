#!/usr/bin/env python3
"""
o3 Enhanced Memory System - ã‚»ãƒƒã‚·ãƒ§ãƒ³é–“è¨˜æ†¶å¼•ãç¶™ãã‚·ã‚¹ãƒ†ãƒ 
æœ€é©åŒ–ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ç¶™ç¶šã‚·ã‚¹ãƒ†ãƒ  with o3 API integration
"""

import json
import os
import asyncio
import aiohttp
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import sqlite3
import hashlib
import logging
from dataclasses import dataclass, asdict
from enum import Enum
import openai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MemoryImportance(Enum):
    """è¨˜æ†¶ã®é‡è¦åº¦ãƒ¬ãƒ™ãƒ«"""
    CRITICAL = 5    # å¿…é ˆç¶™æ‰¿ï¼ˆ78å›ãƒŸã‚¹è¨˜éŒ²ã€è·å‹™å®£è¨€ç­‰ï¼‰
    HIGH = 4        # é‡è¦ã‚¿ã‚¹ã‚¯ãƒ»ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ–‡è„ˆ
    MEDIUM = 3      # ä¸€èˆ¬çš„ä½œæ¥­å±¥æ­´
    LOW = 2         # å‚è€ƒæƒ…å ±
    ARCHIVE = 1     # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å€™è£œ

@dataclass
class MemoryRecord:
    """è¨˜æ†¶ãƒ¬ã‚³ãƒ¼ãƒ‰æ§‹é€ """
    id: str
    session_id: str
    timestamp: datetime
    content: str
    importance: MemoryImportance
    keywords: List[str]
    context_type: str  # 'task', 'conversation', 'mistake', 'directive'
    embedding: Optional[List[float]] = None
    ai_source: str = "claude"  # 'claude', 'gemini', 'o3'
    
class O3EnhancedMemorySystem:
    """o3 APIçµ±åˆè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, 
                 base_path: str = "/Users/dd/Desktop/1_dev/coding-rule2/memory/enhanced",
                 openai_api_key: str = None):
        self.base_path = Path(base_path)
        self.openai_client = openai.AsyncOpenAI(api_key=openai_api_key or os.getenv("OPENAI_API_KEY"))
        self.setup_directories()
        self.init_database()
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        
        # é‡è¦åº¦åˆ¥è¨˜æ†¶å®¹é‡åˆ¶é™
        self.memory_limits = {
            MemoryImportance.CRITICAL: 1000,  # ç„¡åˆ¶é™ã«è¿‘ã„
            MemoryImportance.HIGH: 500,
            MemoryImportance.MEDIUM: 200,
            MemoryImportance.LOW: 100,
            MemoryImportance.ARCHIVE: 50
        }
        
    def setup_directories(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ è¨­å®š"""
        dirs = [
            "session-records",
            "memory-vectors", 
            "ai-collaboration",
            "context-summaries",
            "mistake-prevention",
            "priority-cache",
            "o3-insights"
        ]
        for dir_name in dirs:
            (self.base_path / dir_name).mkdir(parents=True, exist_ok=True)
            
    def init_database(self):
        """æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        db_path = self.base_path / "enhanced_memory.db"
        self.conn = sqlite3.connect(str(db_path), check_same_thread=False)
        
        # æ‹¡å¼µè¨˜æ†¶ãƒ†ãƒ¼ãƒ–ãƒ«
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS enhanced_memories (
                id TEXT PRIMARY KEY,
                session_id TEXT,
                timestamp TEXT,
                content TEXT,
                importance INTEGER,
                keywords TEXT,
                context_type TEXT,
                embedding BLOB,
                ai_source TEXT,
                access_count INTEGER DEFAULT 0,
                last_accessed TEXT,
                relevance_score REAL DEFAULT 0.0
            )
        """)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™æ‰¿ãƒ†ãƒ¼ãƒ–ãƒ«
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS session_inheritance (
                id TEXT PRIMARY KEY,
                previous_session_id TEXT,
                current_session_id TEXT,
                inherited_memories TEXT,
                inheritance_timestamp TEXT,
                inheritance_score REAL
            )
        """)
        
        # AIé€£æºè¨˜éŒ²ãƒ†ãƒ¼ãƒ–ãƒ«
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS ai_collaborations (
                id TEXT PRIMARY KEY,
                session_id TEXT,
                ai_source TEXT,
                interaction_type TEXT,
                input_query TEXT,
                output_result TEXT,
                timestamp TEXT,
                usefulness_score REAL
            )
        """)
        
        # é‡è¦åº¦å­¦ç¿’ãƒ†ãƒ¼ãƒ–ãƒ«
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS importance_learning (
                id TEXT PRIMARY KEY,
                content_pattern TEXT,
                learned_importance INTEGER,
                confidence_score REAL,
                learning_timestamp TEXT
            )
        """)
        
        self.conn.commit()
        
    async def save_memory_with_o3_enhancement(self, 
                                            content: str,
                                            session_id: str,
                                            context_type: str = "conversation",
                                            ai_source: str = "claude") -> str:
        """o3å¼·åŒ–è¨˜æ†¶ä¿å­˜"""
        
        # 1. åŸºæœ¬è¨˜æ†¶ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
        memory_id = hashlib.md5(f"{content}{datetime.now().isoformat()}".encode()).hexdigest()
        
        # 2. o3ã«ã‚ˆã‚‹é‡è¦åº¦ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        importance, keywords = await self._analyze_with_o3(content, context_type)
        
        # 3. åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ
        embedding = await self._generate_embedding(content)
        
        # 4. è¨˜æ†¶ãƒ¬ã‚³ãƒ¼ãƒ‰ä¿å­˜
        memory_record = MemoryRecord(
            id=memory_id,
            session_id=session_id,
            timestamp=datetime.now(),
            content=content,
            importance=importance,
            keywords=keywords,
            context_type=context_type,
            embedding=embedding,
            ai_source=ai_source
        )
        
        self._save_memory_record(memory_record)
        
        logger.info(f"Memory saved with o3 enhancement: {memory_id} (importance: {importance.name})")
        return memory_id
        
    async def _analyze_with_o3(self, content: str, context_type: str) -> Tuple[MemoryImportance, List[str]]:
        """o3ã«ã‚ˆã‚‹å†…å®¹åˆ†æ"""
        try:
            analysis_prompt = f"""
            Analyze the following content and provide:
            1. Importance level (1-5 scale)
            2. Key keywords (max 10)
            
            Content: {content}
            Context: {context_type}
            
            Consider:
            - Mission-critical information (mistakes, directives) = 5
            - Project tasks and decisions = 4
            - General work progress = 3
            - Reference information = 2
            - Archive candidates = 1
            
            Respond in JSON format:
            {{
                "importance": number,
                "keywords": ["keyword1", "keyword2", ...],
                "reasoning": "brief explanation"
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model="o3-mini",
                messages=[
                    {"role": "system", "content": "You are an expert at analyzing content importance for AI memory systems."},
                    {"role": "user", "content": analysis_prompt}
                ],
                max_tokens=300
            )
            
            analysis = json.loads(response.choices[0].message.content)
            importance = MemoryImportance(analysis.get("importance", 3))
            keywords = analysis.get("keywords", [])
            
            return importance, keywords
            
        except Exception as e:
            logger.error(f"o3 analysis failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬åˆ†æ
            return self._fallback_analysis(content, context_type)
            
    def _fallback_analysis(self, content: str, context_type: str) -> Tuple[MemoryImportance, List[str]]:
        """o3å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æ"""
        # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
        critical_keywords = ["ãƒŸã‚¹", "mistake", "error", "å®£è¨€", "directive", "ç¦æ­¢", "å¿…é ˆ"]
        high_keywords = ["å®Ÿè£…", "ã‚·ã‚¹ãƒ†ãƒ ", "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ", "ã‚¿ã‚¹ã‚¯", "å®Œäº†"]
        
        content_lower = content.lower()
        
        # é‡è¦åº¦åˆ¤å®š
        if any(kw in content_lower for kw in critical_keywords):
            importance = MemoryImportance.CRITICAL
        elif any(kw in content_lower for kw in high_keywords):
            importance = MemoryImportance.HIGH
        else:
            importance = MemoryImportance.MEDIUM
            
        # ç°¡æ˜“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = [kw for kw in critical_keywords + high_keywords if kw in content_lower]
        
        return importance, keywords
        
    async def _generate_embedding(self, content: str) -> List[float]:
        """åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆ"""
        try:
            response = await self.openai_client.embeddings.create(
                model="text-embedding-3-small",
                input=content
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            return []
            
    def _save_memory_record(self, memory_record: MemoryRecord):
        """è¨˜æ†¶ãƒ¬ã‚³ãƒ¼ãƒ‰DBä¿å­˜"""
        embedding_blob = json.dumps(memory_record.embedding) if memory_record.embedding else None
        
        self.conn.execute("""
            INSERT OR REPLACE INTO enhanced_memories
            (id, session_id, timestamp, content, importance, keywords, context_type, embedding, ai_source)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            memory_record.id,
            memory_record.session_id,
            memory_record.timestamp.isoformat(),
            memory_record.content,
            memory_record.importance.value,
            json.dumps(memory_record.keywords),
            memory_record.context_type,
            embedding_blob,
            memory_record.ai_source
        ))
        self.conn.commit()
        
    async def inherit_session_memory(self, 
                                   previous_session_id: str,
                                   current_session_id: str) -> Dict[str, Any]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨˜æ†¶ç¶™æ‰¿"""
        logger.info(f"Inheriting memory from {previous_session_id} to {current_session_id}")
        
        # 1. å‰å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®é‡è¦è¨˜æ†¶å–å¾—
        critical_memories = self._get_memories_by_importance(
            previous_session_id, [MemoryImportance.CRITICAL, MemoryImportance.HIGH]
        )
        
        # 2. o3ã«ã‚ˆã‚‹è¨˜æ†¶è¦ç´„ãƒ»å†æ§‹æˆ
        memory_summary = await self._generate_memory_summary(critical_memories)
        
        # 3. ç¶™æ‰¿è¨˜éŒ²ä½œæˆ
        inheritance_id = hashlib.md5(f"{previous_session_id}-{current_session_id}".encode()).hexdigest()
        
        self.conn.execute("""
            INSERT INTO session_inheritance
            (id, previous_session_id, current_session_id, inherited_memories, inheritance_timestamp, inheritance_score)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            inheritance_id,
            previous_session_id,
            current_session_id,
            json.dumps([m.id for m in critical_memories]),
            datetime.now().isoformat(),
            len(critical_memories) / 10.0  # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢
        ))
        self.conn.commit()
        
        # 4. ç¶™æ‰¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        inheritance_context = {
            "previous_session_id": previous_session_id,
            "inherited_memories_count": len(critical_memories),
            "memory_summary": memory_summary,
            "critical_directives": [m.content for m in critical_memories if m.importance == MemoryImportance.CRITICAL],
            "high_priority_tasks": [m.content for m in critical_memories if m.importance == MemoryImportance.HIGH],
            "continuation_points": await self._identify_continuation_points(critical_memories)
        }
        
        logger.info(f"Session inheritance completed: {len(critical_memories)} memories inherited")
        return inheritance_context
        
    def _get_memories_by_importance(self, 
                                  session_id: str,
                                  importance_levels: List[MemoryImportance]) -> List[MemoryRecord]:
        """é‡è¦åº¦åˆ¥è¨˜æ†¶å–å¾—"""
        importance_values = [imp.value for imp in importance_levels]
        placeholders = ','.join(['?'] * len(importance_values))
        
        cursor = self.conn.execute(f"""
            SELECT * FROM enhanced_memories 
            WHERE session_id = ? AND importance IN ({placeholders})
            ORDER BY importance DESC, timestamp DESC
        """, [session_id] + importance_values)
        
        memories = []
        for row in cursor.fetchall():
            memories.append(MemoryRecord(
                id=row[0],
                session_id=row[1],
                timestamp=datetime.fromisoformat(row[2]),
                content=row[3],
                importance=MemoryImportance(row[4]),
                keywords=json.loads(row[5]),
                context_type=row[6],
                embedding=json.loads(row[7]) if row[7] else None,
                ai_source=row[8]
            ))
            
        return memories
        
    async def _generate_memory_summary(self, memories: List[MemoryRecord]) -> str:
        """è¨˜æ†¶è¦ç´„ç”Ÿæˆ"""
        if not memories:
            return "å‰å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ã®ç¶™æ‰¿è¨˜æ†¶ãªã—"
            
        try:
            content_summary = "\n".join([f"- {m.content[:200]}..." for m in memories[:10]])
            
            summary_prompt = f"""
            ä»¥ä¸‹ã®è¨˜æ†¶å†…å®¹ã‹ã‚‰ã€æ¬¡ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§å¿…è¦ãªè¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

            {content_summary}

            è¦ç´„è¦ä»¶ï¼š
            1. é‡è¦ãªæŒ‡ç¤ºãƒ»ç¦æ­¢äº‹é …
            2. æœªå®Œäº†ã‚¿ã‚¹ã‚¯ã®ç¶™ç¶šç‚¹
            3. é‡è¦ãªå­¦ç¿’ãƒ»æ±ºå®šäº‹é …
            4. é¿ã‘ã‚‹ã¹ããƒŸã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³

            ç°¡æ½”ã§å®Ÿç”¨çš„ãªè¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
            """
            
            response = await self.openai_client.chat.completions.create(
                model="o3-mini",
                messages=[
                    {"role": "system", "content": "You are an expert at creating concise, actionable memory summaries for AI systems."},
                    {"role": "user", "content": summary_prompt}
                ],
                max_tokens=500
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Memory summary generation failed: {e}")
            return "è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼ - æ‰‹å‹•ç¢ºèªãŒå¿…è¦"
            
    async def _identify_continuation_points(self, memories: List[MemoryRecord]) -> List[str]:
        """ä½œæ¥­ç¶™ç¶šç‚¹ã®ç‰¹å®š"""
        task_memories = [m for m in memories if m.context_type == "task"]
        if not task_memories:
            return []
            
        try:
            task_content = "\n".join([f"- {m.content}" for m in task_memories])
            
            continuation_prompt = f"""
            ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯å±¥æ­´ã‹ã‚‰ã€æ¬¡ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ç¶™ç¶šã™ã¹ãä½œæ¥­ç‚¹ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š

            {task_content}

            ç‰¹å®šé …ç›®ï¼š
            1. æœªå®Œäº†ã‚¿ã‚¹ã‚¯
            2. ä¸­æ–­ç‚¹
            3. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
            4. ç¢ºèªãŒå¿…è¦ãªäº‹é …

            JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
            {{
                "continuation_points": ["point1", "point2", ...],
                "next_actions": ["action1", "action2", ...],
                "verification_needed": ["item1", "item2", ...]
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model="o3-mini",
                messages=[
                    {"role": "system", "content": "You are an expert at identifying task continuation points."},
                    {"role": "user", "content": continuation_prompt}
                ],
                max_tokens=400
            )
            
            continuation_data = json.loads(response.choices[0].message.content)
            return continuation_data.get("continuation_points", [])
            
        except Exception as e:
            logger.error(f"Continuation point identification failed: {e}")
            return ["ç¶™ç¶šç‚¹ç‰¹å®šã‚¨ãƒ©ãƒ¼ - æ‰‹å‹•ç¢ºèªãŒå¿…è¦"]
            
    async def search_relevant_memories(self, 
                                     query: str,
                                     session_id: str = None,
                                     limit: int = 10) -> List[MemoryRecord]:
        """é–¢é€£è¨˜æ†¶æ¤œç´¢"""
        # 1. ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = await self._generate_embedding(query)
        if not query_embedding:
            return []
            
        # 2. é¡ä¼¼è¨˜æ†¶æ¤œç´¢
        all_memories = self._get_all_memories(session_id)
        relevant_memories = []
        
        for memory in all_memories:
            if memory.embedding:
                similarity = self._calculate_similarity(query_embedding, memory.embedding)
                if similarity > 0.7:  # é¡ä¼¼åº¦é–¾å€¤
                    memory.relevance_score = similarity
                    relevant_memories.append(memory)
                    
        # 3. é‡è¦åº¦ã¨é¡ä¼¼åº¦ã«ã‚ˆã‚‹ä¸¦ã³æ›¿ãˆ
        relevant_memories.sort(key=lambda x: (x.importance.value, x.relevance_score), reverse=True)
        
        return relevant_memories[:limit]
        
    def _get_all_memories(self, session_id: str = None) -> List[MemoryRecord]:
        """å…¨è¨˜æ†¶å–å¾—"""
        if session_id:
            cursor = self.conn.execute("""
                SELECT * FROM enhanced_memories 
                WHERE session_id = ?
                ORDER BY timestamp DESC
            """, (session_id,))
        else:
            cursor = self.conn.execute("""
                SELECT * FROM enhanced_memories 
                ORDER BY timestamp DESC
            """)
            
        memories = []
        for row in cursor.fetchall():
            memories.append(MemoryRecord(
                id=row[0],
                session_id=row[1],
                timestamp=datetime.fromisoformat(row[2]),
                content=row[3],
                importance=MemoryImportance(row[4]),
                keywords=json.loads(row[5]),
                context_type=row[6],
                embedding=json.loads(row[7]) if row[7] else None,
                ai_source=row[8]
            ))
            
        return memories
        
    def _calculate_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        try:
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        except Exception:
            return 0.0
            
    async def generate_startup_context(self, current_session_id: str) -> Dict[str, Any]:
        """èµ·å‹•æ™‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        # 1. å‰å›ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç‰¹å®š
        previous_session = self._get_latest_session(exclude_current=current_session_id)
        
        # 2. è¨˜æ†¶ç¶™æ‰¿å®Ÿè¡Œ
        if previous_session:
            inheritance_context = await self.inherit_session_memory(
                previous_session, current_session_id
            )
        else:
            inheritance_context = {"message": "åˆå›ã‚»ãƒƒã‚·ãƒ§ãƒ³"}
            
        # 3. å¿…é ˆè¨˜æ†¶ã®å–å¾—
        critical_memories = self._get_memories_by_importance(
            None, [MemoryImportance.CRITICAL]
        )
        
        # 4. æœªå®Œäº†ã‚¿ã‚¹ã‚¯ã®å–å¾—
        pending_tasks = await self._get_pending_tasks()
        
        # 5. èµ·å‹•æ™‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        startup_context = {
            "session_id": current_session_id,
            "inheritance": inheritance_context,
            "critical_directives": [m.content for m in critical_memories],
            "pending_tasks": pending_tasks,
            "mistake_prevention_rules": self._get_mistake_prevention_rules(),
            "ai_collaboration_history": self._get_ai_collaboration_summary(),
            "startup_timestamp": datetime.now().isoformat()
        }
        
        return startup_context
        
    def _get_latest_session(self, exclude_current: str = None) -> Optional[str]:
        """æœ€æ–°ã‚»ãƒƒã‚·ãƒ§ãƒ³IDå–å¾—"""
        if exclude_current:
            cursor = self.conn.execute("""
                SELECT DISTINCT session_id FROM enhanced_memories 
                WHERE session_id != ?
                ORDER BY timestamp DESC 
                LIMIT 1
            """, (exclude_current,))
        else:
            cursor = self.conn.execute("""
                SELECT DISTINCT session_id FROM enhanced_memories 
                ORDER BY timestamp DESC 
                LIMIT 1
            """)
            
        result = cursor.fetchone()
        return result[0] if result else None
        
    async def _get_pending_tasks(self) -> List[str]:
        """æœªå®Œäº†ã‚¿ã‚¹ã‚¯å–å¾—"""
        task_memories = self._get_memories_by_importance(
            None, [MemoryImportance.HIGH, MemoryImportance.MEDIUM]
        )
        
        # ã‚¿ã‚¹ã‚¯é–¢é€£è¨˜æ†¶ã‹ã‚‰æœªå®Œäº†é …ç›®ã‚’æŠ½å‡º
        pending_tasks = []
        for memory in task_memories:
            if any(keyword in memory.content.lower() for keyword in 
                   ["æœªå®Œäº†", "ç¶™ç¶š", "todo", "pending", "é€²è¡Œä¸­"]):
                pending_tasks.append(memory.content)
                
        return pending_tasks[:10]  # æœ€å¤§10å€‹
        
    def _get_mistake_prevention_rules(self) -> List[str]:
        """ãƒŸã‚¹é˜²æ­¢ãƒ«ãƒ¼ãƒ«å–å¾—"""
        mistake_memories = [m for m in self._get_all_memories() 
                          if m.context_type == "mistake"]
        
        rules = []
        for memory in mistake_memories:
            rules.append(f"ã€é˜²æ­¢ãƒ«ãƒ¼ãƒ«ã€‘{memory.content}")
            
        return rules[:5]  # æœ€å¤§5å€‹
        
    def _get_ai_collaboration_summary(self) -> Dict[str, Any]:
        """AIé€£æºå±¥æ­´è¦ç´„"""
        cursor = self.conn.execute("""
            SELECT ai_source, COUNT(*) as count, AVG(usefulness_score) as avg_usefulness
            FROM ai_collaborations 
            GROUP BY ai_source
        """)
        
        collaboration_summary = {}
        for row in cursor.fetchall():
            collaboration_summary[row[0]] = {
                "interaction_count": row[1],
                "average_usefulness": row[2] or 0.0
            }
            
        return collaboration_summary

# ä½¿ç”¨ä¾‹
async def main():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼å–å¾—
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
        
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    memory_system = O3EnhancedMemorySystem(openai_api_key=api_key)
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚»ãƒƒã‚·ãƒ§ãƒ³ID
    test_session_id = f"test-session-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    
    # 1. è¨˜æ†¶ä¿å­˜ãƒ†ã‚¹ãƒˆ
    print("ğŸ§  è¨˜æ†¶ä¿å­˜ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    await memory_system.save_memory_with_o3_enhancement(
        "AIæ°¸ç¶šè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚78å›ã®ãƒŸã‚¹ã‚’é˜²ããŸã‚ã€å³æ ¼ãªæ¤œè¨¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚",
        test_session_id,
        "task",
        "claude"
    )
    
    # 2. èµ·å‹•æ™‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    print("ğŸš€ èµ·å‹•æ™‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ...")
    context = await memory_system.generate_startup_context(test_session_id)
    print(f"âœ… ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: {len(context)} é …ç›®")
    
    # 3. é–¢é€£è¨˜æ†¶æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    print("ğŸ” é–¢é€£è¨˜æ†¶æ¤œç´¢ãƒ†ã‚¹ãƒˆ...")
    relevant = await memory_system.search_relevant_memories(
        "è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…", test_session_id
    )
    print(f"âœ… é–¢é€£è¨˜æ†¶æ¤œç´¢å®Œäº†: {len(relevant)} ä»¶")
    
    print("ğŸ¯ o3 Enhanced Memory System ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    asyncio.run(main())