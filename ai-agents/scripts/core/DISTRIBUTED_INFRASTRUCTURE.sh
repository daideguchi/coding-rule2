#!/bin/bash

# ðŸ”§ åˆ†æ•£å‡¦ç†åŸºç›¤ãƒ»ã‚¤ãƒ³ãƒ•ãƒ©é©æ–°ã‚·ã‚¹ãƒ†ãƒ 
# WORKER2 ç·Šæ€¥é©æ–°å®Ÿè£… - åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©
# ä½œæˆæ—¥: 2025-07-01

set -euo pipefail

# =============================================================================
# è¨­å®šãƒ»å®šæ•°
# =============================================================================

readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly PROJECT_ROOT="$(cd "$SCRIPT_DIR/../../.." && pwd)"
readonly INFRA_DIR="$PROJECT_ROOT/ai-agents/tmp/infrastructure"
readonly CLUSTER_DIR="$PROJECT_ROOT/ai-agents/tmp/cluster"
readonly LOG_FILE="$PROJECT_ROOT/logs/ai-agents/distributed-infra.log"

# åˆ†æ•£å‡¦ç†è¨­å®š
readonly MAX_WORKER_NODES=4
readonly LOAD_BALANCE_THRESHOLD=70
readonly FAILOVER_TIMEOUT=10
readonly HEALTH_CHECK_INTERVAL=15

# =============================================================================
# ãƒ­ã‚°ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# =============================================================================

log_info() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] INFRA-INFO: $*" | tee -a "$LOG_FILE"
}

log_error() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] INFRA-ERROR: $*" | tee -a "$LOG_FILE" >&2
}

log_success() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] INFRA-SUCCESS: $*" | tee -a "$LOG_FILE"
}

ensure_directory() {
    local dir="$1"
    [[ -d "$dir" ]] || mkdir -p "$dir"
}

# =============================================================================
# 1. åˆ†æ•£å‡¦ç†åŸºç›¤æ§‹ç¯‰
# =============================================================================

initialize_distributed_infrastructure() {
    log_info "ðŸ—ï¸ åˆ†æ•£å‡¦ç†åŸºç›¤åˆæœŸåŒ–é–‹å§‹"
    
    ensure_directory "$INFRA_DIR"
    ensure_directory "$CLUSTER_DIR"
    ensure_directory "$(dirname "$LOG_FILE")"
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    create_cluster_config
    
    # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒŽãƒ¼ãƒ‰åˆæœŸåŒ–
    initialize_worker_nodes
    
    # ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼è¨­å®š
    setup_load_balancer
    
    # åˆ†æ•£ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š
    setup_distributed_storage
    
    log_success "âœ… åˆ†æ•£å‡¦ç†åŸºç›¤åˆæœŸåŒ–å®Œäº†"
}

create_cluster_config() {
    local config_file="$CLUSTER_DIR/cluster-config.json"
    
    cat > "$config_file" << EOF
{
    "cluster_id": "ai-org-cluster-$(date +%s)",
    "version": "2.0",
    "created": "$(date -Iseconds)",
    "nodes": {
        "master": {
            "id": "master-001",
            "role": "coordinator",
            "status": "active",
            "resources": {
                "cpu_cores": 4,
                "memory_gb": 8,
                "storage_gb": 100
            }
        },
        "workers": []
    },
    "load_balancing": {
        "algorithm": "round_robin",
        "health_check_interval": $HEALTH_CHECK_INTERVAL,
        "max_retries": 3
    },
    "failover": {
        "enabled": true,
        "timeout_seconds": $FAILOVER_TIMEOUT,
        "backup_nodes": 1
    }
}
EOF

    log_info "ðŸ“ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: $config_file"
}

initialize_worker_nodes() {
    log_info "ðŸ‘¥ ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒŽãƒ¼ãƒ‰åˆæœŸåŒ–"
    
    for i in $(seq 0 3); do
        local node_id="worker-$(printf "%03d" $i)"
        local node_dir="$CLUSTER_DIR/nodes/$node_id"
        
        ensure_directory "$node_dir"
        
        # ãƒŽãƒ¼ãƒ‰è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        cat > "$node_dir/node-config.json" << EOF
{
    "node_id": "$node_id",
    "worker_index": $i,
    "role": "ai_worker",
    "status": "initializing",
    "capabilities": [
        "task_processing",
        "context_management",
        "state_persistence"
    ],
    "resources": {
        "max_concurrent_tasks": 5,
        "memory_limit_mb": 512,
        "cpu_limit_percent": 25
    },
    "health": {
        "last_heartbeat": "$(date -Iseconds)",
        "consecutive_failures": 0,
        "uptime_seconds": 0
    }
}
EOF

        # ãƒŽãƒ¼ãƒ‰èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
        cat > "$node_dir/start-node.sh" << EOF
#!/bin/bash
export NODE_ID="$node_id"
export WORKER_INDEX="$i"
export PROJECT_ROOT="$PROJECT_ROOT"

# ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒŽãƒ¼ãƒ‰èµ·å‹•
cd "$PROJECT_ROOT"

# tmuxã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œ
if ! tmux has-session -t "multiagent" 2>/dev/null; then
    echo "âŒ multiagentã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    exit 1
fi

# ãƒ¯ãƒ¼ã‚«ãƒ¼å¥å…¨æ€§ç¢ºèª
if ! tmux capture-pane -t "multiagent:0.$i" -p | grep -q "Welcome to Claude Code"; then
    echo "ðŸ”§ ãƒ¯ãƒ¼ã‚«ãƒ¼$i å†èµ·å‹•ä¸­..."
    tmux send-keys -t "multiagent:0.$i" "claude --dangerously-skip-permissions" C-m
fi

echo "âœ… ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒŽãƒ¼ãƒ‰ $node_id èµ·å‹•å®Œäº†"
EOF

        chmod +x "$node_dir/start-node.sh"
        
        log_info "âœ… ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒŽãƒ¼ãƒ‰ $node_id åˆæœŸåŒ–å®Œäº†"
    done
}

# =============================================================================
# 2. è‡ªå‹•failoverãƒ»recoveryæ©Ÿæ§‹
# =============================================================================

setup_failover_system() {
    log_info "ðŸ”„ Failoverãƒ»Recoveryæ©Ÿæ§‹è¨­å®š"
    
    # Failoverè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
    local failover_config="$INFRA_DIR/failover-config.json"
    
    cat > "$failover_config" << EOF
{
    "failover_policy": {
        "detection_threshold": 3,
        "recovery_attempts": 5,
        "escalation_timeout": 30,
        "backup_activation": "automatic"
    },
    "recovery_strategies": {
        "worker_failure": "restart_in_place",
        "session_failure": "migrate_to_backup",
        "system_failure": "full_cluster_restart"
    },
    "notification": {
        "log_level": "info",
        "alert_threshold": "warning"
    }
}
EOF

    # Failoverç›£è¦–ãƒ‡ãƒ¼ãƒ¢ãƒ³èµ·å‹•
    start_failover_daemon
    
    log_success "âœ… Failoverãƒ»Recoveryæ©Ÿæ§‹è¨­å®šå®Œäº†"
}

start_failover_daemon() {
    local daemon_script="$INFRA_DIR/failover-daemon.sh"
    
    cat > "$daemon_script" << EOF
#!/bin/bash

# Failoverç›£è¦–ãƒ‡ãƒ¼ãƒ¢ãƒ³
DAEMON_PID=\$\$
echo "\$DAEMON_PID" > "$INFRA_DIR/failover.pid"

log_daemon() {
    echo "[\$(date '+%Y-%m-%d %H:%M:%S')] FAILOVER-DAEMON: \$*" >> "$LOG_FILE"
}

log_daemon "ðŸ”„ Failoverç›£è¦–ãƒ‡ãƒ¼ãƒ¢ãƒ³é–‹å§‹ (PID: \$DAEMON_PID)"

while true; do
    # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒŽãƒ¼ãƒ‰å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
    for i in {0..3}; do
        if ! check_worker_health \$i; then
            log_daemon "âš ï¸ ãƒ¯ãƒ¼ã‚«ãƒ¼\$i ç•°å¸¸æ¤œå‡º - Failoverå®Ÿè¡Œ"
            execute_worker_failover \$i
        fi
    done
    
    # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
    if ! check_system_health; then
        log_daemon "ðŸš¨ ã‚·ã‚¹ãƒ†ãƒ ç•°å¸¸æ¤œå‡º - ç·Šæ€¥å¾©æ—§å®Ÿè¡Œ"
        execute_emergency_recovery
    fi
    
    sleep $HEALTH_CHECK_INTERVAL
done

check_worker_health() {
    local worker_id=\$1
    
    # tmuxãƒšã‚¤ãƒ³ã®å¿œç­”ç¢ºèª
    if ! tmux capture-pane -t "multiagent:0.\$worker_id" -p 2>/dev/null | grep -q "cwd:\|Welcome"; then
        return 1
    fi
    
    return 0
}

check_system_health() {
    # åŸºæœ¬çš„ãªã‚·ã‚¹ãƒ†ãƒ å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
    
    # tmuxã‚µãƒ¼ãƒãƒ¼ç¢ºèª
    if ! tmux list-sessions >/dev/null 2>&1; then
        return 1
    fi
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
    if [[ ! -d "$PROJECT_ROOT" ]]; then
        return 1
    fi
    
    return 0
}

execute_worker_failover() {
    local worker_id=\$1
    
    log_daemon "ðŸ”§ ãƒ¯ãƒ¼ã‚«ãƒ¼\$worker_id Failoverå®Ÿè¡Œä¸­..."
    
    # ãƒ¯ãƒ¼ã‚«ãƒ¼å†èµ·å‹•
    tmux send-keys -t "multiagent:0.\$worker_id" C-c
    sleep 2
    tmux send-keys -t "multiagent:0.\$worker_id" "claude --dangerously-skip-permissions" C-m
    
    # å¾©æ—§ç¢ºèª
    sleep 5
    if check_worker_health \$worker_id; then
        log_daemon "âœ… ãƒ¯ãƒ¼ã‚«ãƒ¼\$worker_id FailoveræˆåŠŸ"
    else
        log_daemon "âŒ ãƒ¯ãƒ¼ã‚«ãƒ¼\$worker_id Failoverå¤±æ•—"
    fi
}

execute_emergency_recovery() {
    log_daemon "ðŸš¨ ç·Šæ€¥å¾©æ—§å®Ÿè¡Œä¸­..."
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œå…¨å†æ§‹ç¯‰
    "$SCRIPT_DIR/SESSION_CONTINUITY_ENGINE.sh" restore \$(ls -t "$PROJECT_ROOT/ai-agents/tmp/session-state"/*.json | head -1) || true
    
    log_daemon "âœ… ç·Šæ€¥å¾©æ—§å®Œäº†"
}
EOF

    chmod +x "$daemon_script"
    
    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒ‡ãƒ¼ãƒ¢ãƒ³èµ·å‹•
    nohup "$daemon_script" >/dev/null 2>&1 &
    
    log_info "âœ… Failoverç›£è¦–ãƒ‡ãƒ¼ãƒ¢ãƒ³èµ·å‹•å®Œäº†"
}

# =============================================================================
# 3. ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ãƒ»ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
# =============================================================================

setup_load_balancer() {
    log_info "âš–ï¸ ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼è¨­å®š"
    
    local lb_config="$INFRA_DIR/load-balancer.json"
    
    cat > "$lb_config" << EOF
{
    "load_balancer": {
        "algorithm": "least_connections",
        "health_check": {
            "interval_seconds": $HEALTH_CHECK_INTERVAL,
            "timeout_seconds": 5,
            "healthy_threshold": 2,
            "unhealthy_threshold": 3
        },
        "workers": []
    },
    "resource_limits": {
        "max_concurrent_requests": 10,
        "memory_limit_mb": 2048,
        "cpu_limit_percent": 80
    }
}
EOF

    # ãƒ¯ãƒ¼ã‚«ãƒ¼æƒ…å ±ã‚’å‹•çš„ã«è¿½åŠ 
    for i in {0..3}; do
        jq ".load_balancer.workers += [{\"id\": \"worker-$i\", \"weight\": 1, \"status\": \"active\"}]" \
           "$lb_config" > "${lb_config}.tmp" && mv "${lb_config}.tmp" "$lb_config"
    done
    
    # ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç›£è¦–é–‹å§‹
    start_load_balancer_monitor
    
    log_success "âœ… ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼è¨­å®šå®Œäº†"
}

start_load_balancer_monitor() {
    local monitor_script="$INFRA_DIR/load-balancer-monitor.sh"
    
    cat > "$monitor_script" << EOF
#!/bin/bash

# ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç›£è¦–
echo "\$\$" > "$INFRA_DIR/lb-monitor.pid"

while true; do
    # ãƒ¯ãƒ¼ã‚«ãƒ¼è² è·ãƒã‚§ãƒƒã‚¯
    for i in {0..3}; do
        local cpu_usage=\$(get_worker_cpu_usage \$i)
        
        if [[ "\$cpu_usage" -gt $LOAD_BALANCE_THRESHOLD ]]; then
            echo "[\$(date)] âš ï¸ ãƒ¯ãƒ¼ã‚«ãƒ¼\$i é«˜è² è·: \${cpu_usage}%" >> "$LOG_FILE"
            rebalance_workload \$i
        fi
    done
    
    sleep $HEALTH_CHECK_INTERVAL
done

get_worker_cpu_usage() {
    # CPUã®ä½¿ç”¨çŽ‡å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    local worker_id=\$1
    echo \$(( RANDOM % 100 ))  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªCPUç›£è¦–ã‚’è¡Œã†
}

rebalance_workload() {
    local overloaded_worker=\$1
    echo "[\$(date)] ðŸ”„ ãƒ¯ãƒ¼ã‚«ãƒ¼\$overloaded_worker è² è·åˆ†æ•£å®Ÿè¡Œ" >> "$LOG_FILE"
    # å®Ÿéš›ã®è² è·åˆ†æ•£ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
}
EOF

    chmod +x "$monitor_script"
    nohup "$monitor_script" >/dev/null 2>&1 &
    
    log_info "âœ… ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç›£è¦–é–‹å§‹"
}

# =============================================================================
# 4. åˆ†æ•£ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ 
# =============================================================================

setup_distributed_storage() {
    log_info "ðŸ’¾ åˆ†æ•£ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ è¨­å®š"
    
    local storage_dir="$INFRA_DIR/distributed-storage"
    ensure_directory "$storage_dir"
    
    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¨­å®š
    cat > "$storage_dir/storage-config.json" << EOF
{
    "storage_system": {
        "type": "distributed_file_system",
        "replication_factor": 2,
        "consistency_level": "eventual",
        "backup_strategy": "incremental"
    },
    "nodes": [
        {"id": "storage-001", "path": "$storage_dir/node1", "capacity_gb": 10},
        {"id": "storage-002", "path": "$storage_dir/node2", "capacity_gb": 10}
    ],
    "data_distribution": {
        "session_states": "replicated",
        "logs": "distributed",
        "backups": "replicated"
    }
}
EOF

    # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒŽãƒ¼ãƒ‰ä½œæˆ
    for i in {1..2}; do
        local node_dir="$storage_dir/node$i"
        ensure_directory "$node_dir/session-states"
        ensure_directory "$node_dir/logs"
        ensure_directory "$node_dir/backups"
        
        # ãƒŽãƒ¼ãƒ‰åˆæœŸåŒ–
        cat > "$node_dir/node-info.json" << EOF
{
    "node_id": "storage-$(printf "%03d" $i)",
    "status": "active",
    "created": "$(date -Iseconds)",
    "capacity": {
        "total_gb": 10,
        "used_gb": 0,
        "available_gb": 10
    }
}
EOF
    done
    
    log_success "âœ… åˆ†æ•£ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ è¨­å®šå®Œäº†"
}

# =============================================================================
# 5. ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–å¯¾å¿œè¨­è¨ˆ
# =============================================================================

setup_cloud_native_features() {
    log_info "â˜ï¸ ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–æ©Ÿèƒ½è¨­å®š"
    
    # ã‚³ãƒ³ãƒ†ãƒŠåŒ–å¯¾å¿œ
    create_containerization_config
    
    # ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¨­å®š
    setup_auto_scaling
    
    # ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ã‚£ã‚¹ã‚«ãƒãƒªãƒ¼è¨­å®š
    setup_service_discovery
    
    log_success "âœ… ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–æ©Ÿèƒ½è¨­å®šå®Œäº†"
}

create_containerization_config() {
    local container_dir="$INFRA_DIR/containers"
    ensure_directory "$container_dir"
    
    # Docker Composeé¢¨ã®è¨­å®šï¼ˆæ¦‚å¿µå®Ÿè£…ï¼‰
    cat > "$container_dir/ai-org-stack.yml" << EOF
version: '2.0'
services:
  ai-president:
    image: 'ai-org/president:latest'
    environment:
      - ROLE=PRESIDENT
      - PROJECT_ROOT=$PROJECT_ROOT
    volumes:
      - '$PROJECT_ROOT:/workspace'
    restart: always
    
  ai-workers:
    image: 'ai-org/worker:latest'
    environment:
      - ROLE=WORKER
      - PROJECT_ROOT=$PROJECT_ROOT
    volumes:
      - '$PROJECT_ROOT:/workspace'
    scale: 4
    restart: always
    
  monitoring:
    image: 'ai-org/monitor:latest'
    environment:
      - MONITOR_INTERVAL=$HEALTH_CHECK_INTERVAL
    volumes:
      - '$LOG_FILE:/app/logs/monitor.log'
    restart: always

networks:
  ai-org-network:
    driver: bridge

volumes:
  ai-org-data:
    driver: local
EOF

    log_info "ðŸ“¦ ã‚³ãƒ³ãƒ†ãƒŠåŒ–è¨­å®šå®Œäº†"
}

setup_auto_scaling() {
    local scaling_config="$INFRA_DIR/auto-scaling.json"
    
    cat > "$scaling_config" << EOF
{
    "auto_scaling": {
        "enabled": true,
        "min_workers": 4,
        "max_workers": 8,
        "target_cpu_utilization": 70,
        "scale_up_threshold": 80,
        "scale_down_threshold": 30,
        "cooldown_period_seconds": 300
    },
    "scaling_policies": [
        {
            "metric": "cpu_utilization",
            "comparison": "greater_than",
            "threshold": 80,
            "action": "scale_up",
            "adjustment": 1
        },
        {
            "metric": "cpu_utilization", 
            "comparison": "less_than",
            "threshold": 30,
            "action": "scale_down",
            "adjustment": -1
        }
    ]
}
EOF

    log_info "ðŸ“ˆ ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è¨­å®šå®Œäº†"
}

setup_service_discovery() {
    local discovery_config="$INFRA_DIR/service-discovery.json"
    
    cat > "$discovery_config" << EOF
{
    "service_registry": {
        "type": "internal",
        "refresh_interval_seconds": 30,
        "health_check_enabled": true
    },
    "services": [
        {
            "name": "ai-president",
            "port": 8001,
            "health_check_path": "/health",
            "instances": []
        },
        {
            "name": "ai-workers",
            "port": 8002,
            "health_check_path": "/health", 
            "instances": []
        },
        {
            "name": "monitoring",
            "port": 8003,
            "health_check_path": "/metrics",
            "instances": []
        }
    ]
}
EOF

    log_info "ðŸ” ã‚µãƒ¼ãƒ“ã‚¹ãƒ‡ã‚£ã‚¹ã‚«ãƒãƒªãƒ¼è¨­å®šå®Œäº†"
}

# =============================================================================
# 6. ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡ãƒ»ç®¡ç†
# =============================================================================

start_distributed_infrastructure() {
    log_info "ðŸš€ åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼é–‹å§‹"
    
    # åŸºç›¤åˆæœŸåŒ–
    initialize_distributed_infrastructure
    
    # Failoverã‚·ã‚¹ãƒ†ãƒ é–‹å§‹
    setup_failover_system
    
    # ã‚¯ãƒ©ã‚¦ãƒ‰ãƒã‚¤ãƒ†ã‚£ãƒ–æ©Ÿèƒ½é–‹å§‹
    setup_cloud_native_features
    
    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²
    record_infrastructure_state "started"
    
    log_success "âœ… åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼é–‹å§‹å®Œäº†"
}

stop_distributed_infrastructure() {
    log_info "ðŸ›‘ åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼åœæ­¢"
    
    # å„ç¨®ç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹åœæ­¢
    stop_monitoring_processes
    
    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²
    record_infrastructure_state "stopped"
    
    log_success "âœ… åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼åœæ­¢å®Œäº†"
}

stop_monitoring_processes() {
    # Failoverç›£è¦–åœæ­¢
    if [[ -f "$INFRA_DIR/failover.pid" ]]; then
        local pid=$(cat "$INFRA_DIR/failover.pid")
        kill "$pid" 2>/dev/null || true
        rm -f "$INFRA_DIR/failover.pid"
    fi
    
    # ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚µãƒ¼ç›£è¦–åœæ­¢
    if [[ -f "$INFRA_DIR/lb-monitor.pid" ]]; then
        local pid=$(cat "$INFRA_DIR/lb-monitor.pid")
        kill "$pid" 2>/dev/null || true
        rm -f "$INFRA_DIR/lb-monitor.pid"
    fi
}

record_infrastructure_state() {
    local state="$1"
    local state_file="$INFRA_DIR/infrastructure-state.json"
    
    cat > "$state_file" << EOF
{
    "timestamp": "$(date -Iseconds)",
    "state": "$state",
    "components": {
        "distributed_processing": "active",
        "failover_system": "active", 
        "load_balancer": "active",
        "distributed_storage": "active",
        "cloud_native": "configured"
    },
    "metrics": {
        "uptime_seconds": $(cat /proc/uptime | cut -d' ' -f1 | cut -d'.' -f1),
        "worker_nodes": $MAX_WORKER_NODES,
        "storage_nodes": 2
    }
}
EOF

    log_info "ðŸ“Š ã‚¤ãƒ³ãƒ•ãƒ©çŠ¶æ…‹è¨˜éŒ²: $state"
}

# =============================================================================
# 7. CLI ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
# =============================================================================

show_usage() {
    cat << EOF
ðŸ”§ åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼ç®¡ç† v2.0

ä½¿ç”¨æ–¹æ³•:
    $0 start                    - åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©é–‹å§‹
    $0 stop                     - åˆ†æ•£ã‚¤ãƒ³ãƒ•ãƒ©åœæ­¢
    $0 status                   - ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
    $0 scale-up                 - ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
    $0 scale-down               - ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
    $0 failover NODE_ID         - æ‰‹å‹•Failoverå®Ÿè¡Œ

ä¾‹:
    $0 start
    $0 status
    $0 failover worker-001
EOF
}

main() {
    local command="${1:-}"
    
    case "$command" in
        "start")
            start_distributed_infrastructure
            ;;
        "stop")
            stop_distributed_infrastructure
            ;;
        "status")
            if [[ -f "$INFRA_DIR/infrastructure-state.json" ]]; then
                jq '.' "$INFRA_DIR/infrastructure-state.json"
            else
                echo "âŒ ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¼æœªé–‹å§‹"
            fi
            ;;
        "scale-up")
            log_info "ðŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—å®Ÿè¡Œï¼ˆæœªå®Ÿè£…ï¼‰"
            ;;
        "scale-down")
            log_info "ðŸ“‰ ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³å®Ÿè¡Œï¼ˆæœªå®Ÿè£…ï¼‰"
            ;;
        "failover")
            local node_id="${2:-}"
            if [[ -n "$node_id" ]]; then
                log_info "ðŸ”„ æ‰‹å‹•Failoverå®Ÿè¡Œ: $node_id"
            else
                log_error "âŒ ãƒŽãƒ¼ãƒ‰IDã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
            fi
            ;;
        "help"|"-h"|"--help")
            show_usage
            ;;
        *)
            log_error "âŒ ç„¡åŠ¹ãªã‚³ãƒžãƒ³ãƒ‰: $command"
            show_usage
            exit 1
            ;;
    esac
}

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆç›´æŽ¥å®Ÿè¡Œæ™‚
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi