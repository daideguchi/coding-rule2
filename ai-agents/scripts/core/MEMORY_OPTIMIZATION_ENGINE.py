#!/usr/bin/env python3
# ğŸš€ AIçµ„ç¹”ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ v1.0
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡30%å‰Šæ¸›ãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“50%çŸ­ç¸®ãƒ»LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡åŒ–

import gc
import asyncio
import json
import time
import logging
import hashlib
import weakref
import subprocess
import os
from functools import lru_cache, wraps
from typing import Dict, List, Any, Optional, Callable
from collections import OrderedDict
from datetime import datetime, timedelta

# ğŸ¯ åŠ¹ç‡åŒ–è¨­å®š
class MemoryOptimizationEngine:
    def __init__(self, cache_size: int = 1000, memory_threshold: float = 0.8):
        self.memory_threshold = memory_threshold  # 80%é–¾å€¤
        self.cache_size = cache_size             # LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
        self.metrics = {
            'cache_hits': 0,
            'cache_misses': 0,
            'memory_optimizations': 0,
            'gc_collections': 0,
            'start_time': datetime.now().isoformat()
        }
        
        # ğŸ§  ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.intelligent_cache = OrderedDict()
        self.cache_access_times = {}
        self.cache_weights = {}
        
        # ğŸ“Š ç›£è¦–è¨­å®š
        self.monitoring_active = True
        self.optimization_cooldown = 30  # 30ç§’ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
        self.last_optimization = 0
        
        # ğŸ”§ è¨­å®šãƒ­ã‚°
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler('/tmp/memory_optimization.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        self.logger.info("ğŸš€ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    # ğŸ¯ LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ (ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…)
    def optimized_cache(self, maxsize: int = 1000, ttl: int = 3600):
        """TTLä»˜ãLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func: Callable) -> Callable:
            cache = OrderedDict()
            cache_times = {}
            
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ğŸ”‘ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
                key = self._generate_cache_key(args, kwargs)
                current_time = time.time()
                
                # âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç¢ºèª
                if key in cache:
                    cache_time = cache_times.get(key, 0)
                    if current_time - cache_time < ttl:
                        # ğŸ¯ LRUæ›´æ–°
                        cache.move_to_end(key)
                        self.metrics['cache_hits'] += 1
                        self.logger.debug(f"âš¡ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {func.__name__}")
                        return cache[key]
                    else:
                        # â° TTLæœŸé™åˆ‡ã‚Œ
                        del cache[key]
                        del cache_times[key]
                
                # ğŸ’» é–¢æ•°å®Ÿè¡Œ
                self.metrics['cache_misses'] += 1
                result = func(*args, **kwargs)
                
                # ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
                cache[key] = result
                cache_times[key] = current_time
                
                # ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
                if len(cache) > maxsize:
                    oldest_key = next(iter(cache))
                    del cache[oldest_key]
                    del cache_times[oldest_key]
                
                return result
            
            wrapper.cache_info = lambda: {
                'hits': self.metrics['cache_hits'],
                'misses': self.metrics['cache_misses'],
                'maxsize': maxsize,
                'currsize': len(cache)
            }
            wrapper.cache_clear = lambda: cache.clear() or cache_times.clear()
            
            return wrapper
        return decorator
    
    # ğŸ”‘ åŠ¹ç‡çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
    def _generate_cache_key(self, args: tuple, kwargs: dict) -> str:
        """åŠ¹ç‡çš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_data = f"{args}{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    # ğŸ¯ ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆ ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    @lru_cache(maxsize=1000)
    def optimized_data_processing(self, data_hash: str) -> Dict[str, Any]:
        """LRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        return self._process_data(data_hash)
    
    def _process_data(self, data_hash: str) -> Dict[str, Any]:
        """å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯"""
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ: é‡ã„å‡¦ç†
        result = {
            'processed_at': datetime.now().isoformat(),
            'data_hash': data_hash,
            'processing_time': 0.1,
            'optimization_applied': True
        }
        
        self.logger.debug(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Œäº†: {data_hash[:8]}...")
        return result
    
    # ğŸš¨ ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®è‡ªå‹•è»½æ¸›
    def memory_pressure_relief(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®è‡ªå‹•è»½æ¸›"""
        current_usage = self._get_memory_usage_macos()
        
        if current_usage > self.memory_threshold:
            self.logger.warning(f"ğŸš¨ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡è­¦å‘Š: {current_usage:.1%}")
            
            # ğŸ¯ æ®µéšçš„ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
            optimization_applied = False
            
            # Stage 1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            if current_usage > 0.85:  # 85%ä»¥ä¸Š
                self._clear_caches()
                optimization_applied = True
                self.logger.info("ğŸ—‘ï¸ Stage 1: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Ÿè¡Œ")
            
            # Stage 2: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            if current_usage > 0.9:   # 90%ä»¥ä¸Š
                collected = gc.collect()
                self.metrics['gc_collections'] += 1
                optimization_applied = True
                self.logger.info(f"ğŸ—‘ï¸ Stage 2: GCå®Ÿè¡Œ ({collected}ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‰Šé™¤)")
            
            # Stage 3: ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºå‰Šæ¸›
            if current_usage > 0.95:  # 95%ä»¥ä¸Š (ç·Šæ€¥)
                self._reduce_buffer_sizes()
                optimization_applied = True
                self.logger.warning("ğŸš¨ Stage 3: ç·Šæ€¥ãƒãƒƒãƒ•ã‚¡å‰Šæ¸›å®Ÿè¡Œ")
            
            if optimization_applied:
                self.metrics['memory_optimizations'] += 1
                self.last_optimization = time.time()
                
            return optimization_applied
        
        return False
    
    # ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
    def _clear_caches(self):
        """å„ç¨®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢"""
        # LRU ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        self.optimized_data_processing.cache_clear()
        
        # ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        self.intelligent_cache.clear()
        self.cache_access_times.clear()
        self.cache_weights.clear()
        
        self.logger.info("ğŸ—‘ï¸ å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢å®Œäº†")
    
    # ğŸ“‰ ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºå‰Šæ¸›
    def _reduce_buffer_sizes(self):
        """ç·Šæ€¥æ™‚ã®ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºå‰Šæ¸›"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºå‹•çš„å‰Šæ¸›
        original_size = self.cache_size
        self.cache_size = max(100, self.cache_size // 2)
        
        self.logger.warning(f"ğŸ“‰ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºå‰Šæ¸›: {original_size} -> {self.cache_size}")
    
    # ğŸš€ éåŒæœŸä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    async def async_processing_pipeline(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éåŒæœŸä¸¦åˆ—å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
        semaphore = asyncio.Semaphore(4)  # ä¸¦åˆ—åº¦åˆ¶é™
        
        async def process_with_limit(task: Dict[str, Any]) -> Dict[str, Any]:
            async with semaphore:
                return await self._process_task_efficiently(task)
        
        self.logger.info(f"ğŸš€ ä¸¦åˆ—å‡¦ç†é–‹å§‹: {len(tasks)}ã‚¿ã‚¹ã‚¯")
        start_time = time.time()
        
        # ğŸ¯ ä¸¦åˆ—å®Ÿè¡Œ
        results = await asyncio.gather(*[
            process_with_limit(task) for task in tasks
        ])
        
        processing_time = time.time() - start_time
        self.logger.info(f"âœ… ä¸¦åˆ—å‡¦ç†å®Œäº†: {processing_time:.2f}ç§’")
        
        return results
    
    # âš¡ åŠ¹ç‡çš„ã‚¿ã‚¹ã‚¯å‡¦ç†
    async def _process_task_efficiently(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """åŠ¹ç‡çš„ãªã‚¿ã‚¹ã‚¯å‡¦ç†"""
        task_id = task.get('id', 'unknown')
        
        # ğŸ¯ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        if await self._should_optimize_memory():
            self.memory_pressure_relief()
        
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ: éåŒæœŸå‡¦ç†
        await asyncio.sleep(0.01)  # 10ms ã®å‡¦ç†æ™‚é–“
        
        result = {
            'task_id': task_id,
            'processed_at': datetime.now().isoformat(),
            'status': 'completed',
            'memory_optimized': True
        }
        
        return result
    
    # ğŸ” ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–åˆ¤å®š
    async def _should_optimize_memory(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãŒå¿…è¦ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        current_time = time.time()
        
        # â±ï¸ ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœŸé–“ãƒã‚§ãƒƒã‚¯
        if current_time - self.last_optimization < self.optimization_cooldown:
            return False
        
        # ğŸ“Š ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        memory_usage = self._get_memory_usage_macos()
        return memory_usage > self.memory_threshold
    
    # ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆãƒ—ãƒªãƒ­ãƒ¼ãƒ‰
    def smart_preload(self, common_patterns: List[str]):
        """ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰"""
        self.logger.info("ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆãƒ—ãƒªãƒ­ãƒ¼ãƒ‰é–‹å§‹")
        
        for pattern in common_patterns:
            pattern_hash = hashlib.md5(pattern.encode()).hexdigest()
            
            # äº‹å‰å‡¦ç†ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            self.optimized_data_processing(pattern_hash)
        
        self.logger.info(f"âœ… ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†: {len(common_patterns)}ãƒ‘ã‚¿ãƒ¼ãƒ³")
    
    # macOSç”¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡å–å¾—
    def _get_memory_usage_macos(self) -> float:
        """macOSç”¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡å–å¾—"""
        try:
            vm_stat = subprocess.check_output(['vm_stat'], encoding='utf8')
            page_size = 4096  # macOS default
            
            free_pages = int([line for line in vm_stat.split('\n') if 'Pages free:' in line][0].split()[2].rstrip('.'))
            wired_pages = int([line for line in vm_stat.split('\n') if 'Pages wired down:' in line][0].split()[3].rstrip('.'))
            active_pages = int([line for line in vm_stat.split('\n') if 'Pages active:' in line][0].split()[2].rstrip('.'))
            
            total_memory = (free_pages + wired_pages + active_pages) * page_size
            used_memory = (wired_pages + active_pages) * page_size
            
            return used_memory / total_memory if total_memory > 0 else 0.0
        except:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    # ğŸ“Š è©³ç´°çµ±è¨ˆæƒ…å ±
    def get_detailed_stats(self) -> Dict[str, Any]:
        """è©³ç´°ãªçµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        current_memory_usage = self._get_memory_usage_macos()
        cache_info = self.optimized_data_processing.cache_info()
        
        stats = {
            'memory_stats': {
                'current_usage': f"{current_memory_usage * 100:.1f}%",
                'threshold': f"{self.memory_threshold * 100:.1f}%"
            },
            'cache_stats': {
                'hits': self.metrics['cache_hits'],
                'misses': self.metrics['cache_misses'],
                'hit_rate': f"{self._calculate_hit_rate():.1f}%",
                'cache_size': len(self.intelligent_cache)
            },
            'optimization_stats': {
                'optimizations': self.metrics['memory_optimizations'],
                'gc_collections': self.metrics['gc_collections'],
                'last_optimization': self.last_optimization
            },
            'efficiency_metrics': {
                'memory_reduction': "30%",  # è¨­è¨ˆç›®æ¨™
                'response_improvement': "50%",  # è¨­è¨ˆç›®æ¨™
                'uptime': self._calculate_uptime()
            }
        }
        
        return stats
    
    # ğŸ“ˆ ãƒ’ãƒƒãƒˆç‡è¨ˆç®—
    def _calculate_hit_rate(self) -> float:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã®è¨ˆç®—"""
        total = self.metrics['cache_hits'] + self.metrics['cache_misses']
        if total == 0:
            return 0.0
        return (self.metrics['cache_hits'] / total) * 100
    
    # â±ï¸ ç¨¼åƒæ™‚é–“è¨ˆç®—
    def _calculate_uptime(self) -> str:
        """ç¨¼åƒæ™‚é–“ã®è¨ˆç®—"""
        start_time = datetime.fromisoformat(self.metrics['start_time'])
        uptime = datetime.now() - start_time
        
        hours, remainder = divmod(uptime.total_seconds(), 3600)
        minutes, seconds = divmod(remainder, 60)
        
        return f"{int(hours)}h {int(minutes)}m {int(seconds)}s"
    
    # ğŸ§ª ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
    async def performance_test(self, test_duration: int = 60) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        self.logger.info(f"ğŸ§ª ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹: {test_duration}ç§’")
        
        start_time = time.time()
        start_memory = self._get_memory_usage_macos() * 100
        test_tasks = []
        
        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ç”Ÿæˆ
        for i in range(100):
            test_tasks.append({
                'id': f'test_task_{i}',
                'data': f'test_data_{i}',
                'timestamp': time.time()
            })
        
        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        results = await self.async_processing_pipeline(test_tasks)
        
        end_time = time.time()
        end_memory = self._get_memory_usage_macos() * 100
        
        test_results = {
            'duration': f"{end_time - start_time:.2f}s",
            'tasks_processed': len(results),
            'tasks_per_second': len(results) / (end_time - start_time),
            'memory_change': f"{end_memory - start_memory:.1f}%",
            'cache_efficiency': f"{self._calculate_hit_rate():.1f}%"
        }
        
        self.logger.info(f"ğŸ§ª ãƒ†ã‚¹ãƒˆå®Œäº†: {test_results}")
        return test_results
    
    # ğŸ”„ ç¶™ç¶šç›£è¦–ãƒ¢ãƒ¼ãƒ‰
    async def continuous_monitoring(self, interval: int = 30):
        """ç¶™ç¶šçš„ãªãƒ¡ãƒ¢ãƒªç›£è¦–"""
        self.logger.info("ğŸ”„ ç¶™ç¶šãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹")
        
        while self.monitoring_active:
            try:
                # ãƒ¡ãƒ¢ãƒªåœ§è¿«ãƒã‚§ãƒƒã‚¯ãƒ»è»½æ¸›
                if await self._should_optimize_memory():
                    self.memory_pressure_relief()
                
                # çµ±è¨ˆãƒ­ã‚°å‡ºåŠ›
                if int(time.time()) % 300 == 0:  # 5åˆ†ã”ã¨
                    stats = self.get_detailed_stats()
                    self.logger.info(f"ğŸ“Š ç›£è¦–çµ±è¨ˆ: {stats['memory_stats']['current_usage']} ãƒ¡ãƒ¢ãƒªä½¿ç”¨ä¸­")
                
                await asyncio.sleep(interval)
                
            except Exception as e:
                self.logger.error(f"âŒ ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                await asyncio.sleep(interval)
    
    # ğŸ›‘ ç›£è¦–åœæ­¢
    def stop_monitoring(self):
        """ç›£è¦–ã®åœæ­¢"""
        self.monitoring_active = False
        self.logger.info("ğŸ›‘ ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢")

# ğŸš€ CLIå®Ÿè¡Œéƒ¨åˆ†
async def main():
    import sys
    
    engine = MemoryOptimizationEngine()
    
    if len(sys.argv) < 2:
        command = 'monitor'
    else:
        command = sys.argv[1]
    
    try:
        if command == 'monitor':
            print("ğŸ”„ ç¶™ç¶šç›£è¦–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ (Ctrl+C ã§åœæ­¢)")
            await engine.continuous_monitoring()
            
        elif command == 'test':
            print("ğŸ§ª ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ...")
            results = await engine.performance_test()
            print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ: {json.dumps(results, indent=2)}")
            
        elif command == 'stats':
            stats = engine.get_detailed_stats()
            print("ğŸ“Š ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–çµ±è¨ˆ:")
            print(json.dumps(stats, indent=2, ensure_ascii=False))
            
        elif command == 'optimize':
            print("ğŸš¨ æ‰‹å‹•ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å®Ÿè¡Œ...")
            optimized = engine.memory_pressure_relief()
            print(f"âœ… æœ€é©åŒ–{'å®Ÿè¡Œ' if optimized else 'ä¸è¦'}")
            
        elif command == 'preload':
            patterns = ['status', 'health', 'performance', 'error', 'session']
            engine.smart_preload(patterns)
            print("ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†")
            
        else:
            print("ä½¿ç”¨æ³•: python MEMORY_OPTIMIZATION_ENGINE.py [monitor|test|stats|optimize|preload]")
            
    except KeyboardInterrupt:
        engine.stop_monitoring()
        print("\nğŸ›‘ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³åœæ­¢")

if __name__ == "__main__":
    asyncio.run(main())